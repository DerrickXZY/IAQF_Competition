{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count":29,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"uaIKgIGn2j9E8y7eKu8j2D",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "\n",
    "\n",
    "def get_uv_from_xy(x,y):\n",
    "    x_cdf = ECDF(x)\n",
    "    y_cdf = ECDF(y)\n",
    "    len_x = len(x)\n",
    "    len_y = len(y)\n",
    "    u, v = [len_x\/(len_x+1)*x_cdf(a) for a in x], [len_y\/(len_y+1)*y_cdf(a) for a in y]\n",
    "    return u,v\n",
    "\n",
    "\n",
    "def get_parameter(family, tau):\n",
    "    \"\"\"\n",
    "    Estimate the theta parameter for the copula based on Kendall tau\n",
    "    \"\"\"\n",
    "    if  family == 'clayton':\n",
    "        return 2 * tau \/ (1 - tau)\n",
    "\n",
    "    elif family == 'frank':\n",
    "        integrand = lambda t: t \/ (np.exp(t) - 1)  # generate the integrand\n",
    "        frank_fun = lambda theta: ((tau - 1) \/ 4.0  - (quad(integrand, sys.float_info.epsilon, theta)[0] \/ theta - 1) \/ theta) ** 2\n",
    "        return minimize(frank_fun, 4, method='BFGS', tol=1e-5).x[0] \n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        return 1 \/ (1 - tau)\n",
    "\n",
    "\n",
    "def pdf_copula(family, theta, u, v):\n",
    "    \"\"\"\n",
    "    Estimate the probability density function of three kinds of Archimedean copulas\n",
    "    \"\"\"\n",
    "    if  family == 'clayton':\n",
    "        pdf = (theta+1) * ((u ** (-theta) + v ** (-theta) - 1) ** (-2 - 1\/theta)) * (u ** (-theta-1) * v ** (-theta-1))\n",
    "\n",
    "    elif family == 'frank':\n",
    "        num = -theta * (np.exp(-theta) - 1) * (np.exp(-theta * (u + v)))\n",
    "        denom = ((np.exp(-theta * u) - 1) * (np.exp(-theta * v) - 1) + (np.exp(-theta) - 1)) ** 2\n",
    "        pdf = num \/ denom\n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        A = (-np.log(u)) ** theta + (-np.log(v)) ** theta\n",
    "        c = np.exp(-A ** (1 \/ theta))\n",
    "        pdf = c * (u * v) ** (-1) * (A ** (-2 + 2\/theta)) * ((np.log(u) * np.log(v)) ** (theta - 1)) * (1 + (theta - 1) * A ** (-1\/theta))\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "def log_pdf(family, theta, u, v):\n",
    "    pdf = pdf_copula(family, theta, u, v)\n",
    "    return np.log(pdf)\n",
    "\n",
    "\n",
    "def conditional_cdf(family,theta,u,v):\n",
    "    \"\"\"\n",
    "    This is C(u|v) = dC\/dv \n",
    "    Since u and v are symmetric, reverse the parameters for C(v|u)=dC\/du\n",
    "    \"\"\"\n",
    "    if family == 'clayton':\n",
    "        ccdf =  v ** (-theta-1) * (u ** (-theta) + v ** (-theta) -1) ** (-1\/theta -1)\n",
    "            \n",
    "    elif family == 'frank':\n",
    "        exp_u = np.exp(-theta * u) - 1\n",
    "        exp_v = np.exp(-theta * v) - 1\n",
    "        ccdf = ( exp_u * exp_v + exp_u ) \/ ( exp_u * exp_v + (np.exp(-theta)-1) )\n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        A = (-np.log(u)) ** theta + (-np.log(v)) ** theta\n",
    "        c = np.exp(-A ** (1 \/ theta))\n",
    "        ccdf = c * A ** ((1-theta)\/theta) * (-np.log(v)) ** (theta-1) * (1\/v)\n",
    "\n",
    "    return ccdf\n",
    "\n",
    "\n",
    "def add_copula_x_variable(df, df_price_1, df_price_2, train_start=None, train_end=None, ret_days=1):\n",
    "    \"\"\"\n",
    "\tFit the copula on training data and create a new column with MI_u_v and MI_v_u\n",
    "\t\"\"\"\n",
    "\n",
    "    if not train_start: train_start = df.Date.min()\n",
    "    if not train_end: train_end = df.Date.max()\n",
    "    # Compute total returns\n",
    "    df_price_1 = df_price_1.pct_change(periods=ret_days)\n",
    "    df_price_2 = df_price_2.pct_change(periods=ret_days)\n",
    "    x_fit = df_price_1.loc[train_start : train_end+pd.Timedelta(days=1)].dropna()\n",
    "    y_fit = df_price_2.loc[train_start : train_end+pd.Timedelta(days=1)].dropna()\n",
    "    u_fit,v_fit = get_uv_from_xy(x_fit, y_fit)\n",
    "    tau = kendalltau(x_fit, y_fit)[0]\n",
    "\n",
    "    AIC ={}  # generate a dict with key being the copula family, value = [theta, AIC]\n",
    "    for i in ['clayton', 'frank', 'gumbel']:\n",
    "        param = get_parameter(i, tau)\n",
    "        lpdf = [log_pdf(i, param, a, b) for (a, b) in zip(u_fit, v_fit)]\n",
    "        lpdf = np.nan_to_num(lpdf) \n",
    "        loglikelihood = sum(lpdf)\n",
    "        AIC[i] = [param, -2 * loglikelihood + 2]\n",
    "\n",
    "    fitted_copula = min(AIC.items(), key = lambda a: a[1][1])[0]\n",
    "    fitted_theta = AIC[fitted_copula][0]\n",
    "\n",
    "    start_dt = df.Date.min()\n",
    "    end_dt = df.Date.max()\n",
    "    x = df_price_1.loc[start_dt : end_dt+pd.Timedelta(days=1)].dropna()\n",
    "    y = df_price_2.loc[start_dt : end_dt+pd.Timedelta(days=1)].dropna()\n",
    "    u,v = get_uv_from_xy(x, y)\n",
    "\n",
    "    df_p_val = pd.DataFrame([u, v]).T\n",
    "    df_p_val.index = x.index\n",
    "    df_p_val.index.name = 'Date'\n",
    "    df_p_val.columns = ['u', 'v']\n",
    "    df_p_val['MI_u_v'] = df_p_val.apply(lambda r: conditional_cdf(fitted_copula, fitted_theta, r.u, r.v),axis=1)\n",
    "    df_p_val['MI_v_u'] = df_p_val.apply(lambda r: conditional_cdf(fitted_copula, fitted_theta, r.v, r.u),axis=1)\n",
    "    df_p_val.reset_index(inplace=True)\n",
    "    \n",
    "\n",
    "    df = df.merge(df_p_val[['Date','MI_u_v', 'MI_v_u']], on='Date', how='left')\n",
    "    return df.set_index('Date')\n"
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"oScy8vuCucjZnEKdqOCA7c",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Load Pairs\n",
    "raw_price_df = pd.read_csv(\"Final Price Df.csv\", parse_dates=[\"Date\"])\n",
    "raw_price_df.sort_values([\"ETF_Ticker\", \"Date\"], inplace=True)\n",
    "\n",
    "feature_df_raw = pd.read_csv(\"Final TrainingSet.csv\", parse_dates=[\"Date\"])\n",
    "feature_df_raw.sort_values([\"Ticker_Pair\", \"Date\"], inplace=True)\n",
    "\n",
    "pair_arr = np.unique(feature_df_raw[\"Ticker_Pair\"].values)\n",
    "\n",
    "# Make predictions on each pair under \n",
    "freq_map = {'d':1, 'w':5, 'M':21}\n",
    "price_s = raw_price_df.set_index([\"ETF_Ticker\", \"Date\"]).squeeze()\n",
    "feature_df = feature_df_raw.set_index([\"Ticker_Pair\", \"Date\"]).squeeze()\n",
    "\n",
    "\n",
    "freq = 'M'\n",
    "mode = 'predict'\n",
    "output_list = []\n",
    "\n",
    "for pair in pair_arr:\n",
    "    print(pair)\n",
    "    pair_list = pair.split(\"_\")\n",
    "    pair_s = price_s.loc[pair_list, :].copy()\n",
    "    pair_df = pair_s.unstack(\"ETF_Ticker\")\n",
    "    pair_df.dropna(inplace=True)\n",
    "\n",
    "    feature_df_pair = feature_df.loc[pair, :].copy()\n",
    "\n",
    "    if mode == \"predict\":\n",
    "        feature_df_pair = add_copula_x_variable(feature_df_pair.reset_index(), pair_df[pair_list[0]], pair_df[pair_list[1]],\n",
    "            train_end=pd.Timestamp('2016-12-31'),ret_days=freq_map[freq])\n",
    "        output_i_df = feature_df_pair[['MI_u_v', 'MI_v_u']]\n",
    "        \n",
    "        output_i_df.index.name = \"Date\"\n",
    "        output_i_df.reset_index(inplace=True)\n",
    "        output_i_df[\"pair\"] = pair\n",
    "        output_i_df = output_i_df.reindex([\"pair\"] + list(output_i_df.columns[:-1]), axis=1)\n",
    "        output_list.append(output_i_df)\n",
    "        output_df = pd.concat(output_list)"
   ],
   "execution_count":57,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "*XFN_IJH\n",
      "*XIC_EZA\n",
      "*XIU_REM\n",
      "BKF_IGM\n",
      "BKF_IVV\n",
      "EWD_THD\n",
      "EWG_THD\n",
      "EWH_IJH\n",
      "EWM_EZA\n",
      "EWQ_IYG\n",
      "EWT_IJH\n",
      "EWT_IJJ\n",
      "EWT_IVE\n",
      "EWT_IVV\n",
      "EWT_IWB\n",
      "EWT_IWV\n",
      "EZU_THD\n",
      "IDU_IVV\n",
      "IEV_THD\n",
      "IFGL_IHE\n",
      "IFGL_IWM\n",
      "IGM_IHI\n",
      "IHE_ACWI\n",
      "IHE_ACWX\n",
      "IHE_SCZ\n",
      "IHE_TOK\n",
      "IHE_WOOD\n",
      "IHF_ILCB\n",
      "IHF_ILCG\n",
      "IHF_IUSG\n",
      "IHF_IVW\n",
      "IHF_IXN\n",
      "IJH_ACWI\n",
      "IJH_TOK\n",
      "IJH_WOOD\n",
      "IJJ_ACWI\n",
      "IMCV_IWM\n",
      "IVV_ACWI\n",
      "IWB_ACWI\n",
      "IWM_ACWI\n",
      "IWM_SCZ\n",
      "IWM_SUSA\n",
      "IWM_WOOD\n",
      "IWV_ACWI\n",
      "IXP_IYH\n",
      "IYH_ACWI\n",
      "IYH_TOK\n",
      "IYK_ACWI\n",
      "REM_SUSA\n",
      "RXI_WOOD\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"QBtE64L2a4yrFoPL60swgm",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df.to_pickle(f\"CopulaPredictions_{freq}.pkl\")"
   ],
   "execution_count":58,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"YUmlR5uqhSU92u6KsPdxwp",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}