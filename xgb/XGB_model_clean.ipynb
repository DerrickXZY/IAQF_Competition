{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "\n",
    "\n",
    "def get_uv_from_xy(x,y):\n",
    "    x_cdf = ECDF(x)\n",
    "    y_cdf = ECDF(y)\n",
    "    len_x = len(x)\n",
    "    len_y = len(y)\n",
    "    u, v = [len_x\/(len_x+1)*x_cdf(a) for a in x], [len_y\/(len_y+1)*y_cdf(a) for a in y]\n",
    "    return u,v\n",
    "\n",
    "\n",
    "def get_parameter(family, tau):\n",
    "    \"\"\"\n",
    "    Estimate the theta parameter for the copula based on Kendall tau\n",
    "    \"\"\"\n",
    "    if  family == 'clayton':\n",
    "        return 2 * tau \/ (1 - tau)\n",
    "\n",
    "    elif family == 'frank':\n",
    "        integrand = lambda t: t \/ (np.exp(t) - 1)  # generate the integrand\n",
    "        frank_fun = lambda theta: ((tau - 1) \/ 4.0  - (quad(integrand, sys.float_info.epsilon, theta)[0] \/ theta - 1) \/ theta) ** 2\n",
    "        return minimize(frank_fun, 4, method='BFGS', tol=1e-5).x[0] \n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        return 1 \/ (1 - tau)\n",
    "\n",
    "\n",
    "def pdf_copula(family, theta, u, v):\n",
    "    \"\"\"\n",
    "    Estimate the probability density function of three kinds of Archimedean copulas\n",
    "    \"\"\"\n",
    "    if  family == 'clayton':\n",
    "        pdf = (theta+1) * ((u ** (-theta) + v ** (-theta) - 1) ** (-2 - 1\/theta)) * (u ** (-theta-1) * v ** (-theta-1))\n",
    "\n",
    "    elif family == 'frank':\n",
    "        num = -theta * (np.exp(-theta) - 1) * (np.exp(-theta * (u + v)))\n",
    "        denom = ((np.exp(-theta * u) - 1) * (np.exp(-theta * v) - 1) + (np.exp(-theta) - 1)) ** 2\n",
    "        pdf = num \/ denom\n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        A = (-np.log(u)) ** theta + (-np.log(v)) ** theta\n",
    "        c = np.exp(-A ** (1 \/ theta))\n",
    "        pdf = c * (u * v) ** (-1) * (A ** (-2 + 2\/theta)) * ((np.log(u) * np.log(v)) ** (theta - 1)) * (1 + (theta - 1) * A ** (-1\/theta))\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "\n",
    "def log_pdf(family, theta, u, v):\n",
    "    pdf = pdf_copula(family, theta, u, v)\n",
    "    return np.log(pdf)\n",
    "\n",
    "\n",
    "def conditional_cdf(family,theta,u,v):\n",
    "    \"\"\"\n",
    "    This is C(u|v) = dC\/dv \n",
    "    Since u and v are symmetric, reverse the parameters for C(v|u)=dC\/du\n",
    "    \"\"\"\n",
    "    if family == 'clayton':\n",
    "        ccdf =  v ** (-theta-1) * (u ** (-theta) + v ** (-theta) -1) ** (-1\/theta -1)\n",
    "            \n",
    "    elif family == 'frank':\n",
    "        exp_u = np.exp(-theta * u) - 1\n",
    "        exp_v = np.exp(-theta * v) - 1\n",
    "        ccdf = ( exp_u * exp_v + exp_u ) \/ ( exp_u * exp_v + (np.exp(-theta)-1) )\n",
    "\n",
    "    elif family == 'gumbel':\n",
    "        A = (-np.log(u)) ** theta + (-np.log(v)) ** theta\n",
    "        c = np.exp(-A ** (1 \/ theta))\n",
    "        ccdf = c * A ** ((1-theta)\/theta) * (-np.log(v)) ** (theta-1) * (1\/v)\n",
    "\n",
    "    return ccdf\n",
    "\n",
    "\n",
    "def add_copula_x_variable(df, df_price_1, df_price_2, train_start=None, train_end=None, ret_days=1):\n",
    "    \"\"\"\n",
    "\tFit the copula on training data and create a new column with MI_u_v and MI_v_u\n",
    "\t\"\"\"\n",
    "\n",
    "    # df.Date = pd.to_datetime(df.Date)\n",
    "    if not train_start: train_start = df.Date.min()\n",
    "    if not train_end: train_end = df.Date.max()\n",
    "    # Compute total returns\n",
    "    df_price_1 = df_price_1.pct_change(periods=ret_days)\n",
    "    df_price_2 = df_price_2.pct_change(periods=ret_days)\n",
    "    x_fit = df_price_1.loc[train_start : train_end+pd.Timedelta(days=1)].dropna()\n",
    "    y_fit = df_price_2.loc[train_start : train_end+pd.Timedelta(days=1)].dropna()\n",
    "    u_fit,v_fit = get_uv_from_xy(x_fit, y_fit)\n",
    "    tau = kendalltau(x_fit, y_fit)[0]\n",
    "\n",
    "    AIC ={}  # generate a dict with key being the copula family, value = [theta, AIC]\n",
    "    for i in ['clayton', 'frank', 'gumbel']:\n",
    "        param = get_parameter(i, tau)\n",
    "        lpdf = [log_pdf(i, param, a, b) for (a, b) in zip(u_fit, v_fit)]\n",
    "        lpdf = np.nan_to_num(lpdf) \n",
    "        loglikelihood = sum(lpdf)\n",
    "        AIC[i] = [param, -2 * loglikelihood + 2]\n",
    "\n",
    "    fitted_copula = min(AIC.items(), key = lambda a: a[1][1])[0]\n",
    "    fitted_theta = AIC[fitted_copula][0]\n",
    "\n",
    "    start_dt = df.Date.min()\n",
    "    end_dt = df.Date.max()\n",
    "    x = df_price_1.loc[start_dt : end_dt+pd.Timedelta(days=1)].dropna()\n",
    "    y = df_price_2.loc[start_dt : end_dt+pd.Timedelta(days=1)].dropna()\n",
    "    u,v = get_uv_from_xy(x, y)\n",
    "\n",
    "    df_p_val = pd.DataFrame([u, v]).T\n",
    "    df_p_val.index = x.index\n",
    "    df_p_val.index.name = 'Date'\n",
    "    df_p_val.columns = ['u', 'v']\n",
    "    df_p_val['MI_u_v'] = df_p_val.apply(lambda r: conditional_cdf(fitted_copula, fitted_theta, r.u, r.v),axis=1)\n",
    "    df_p_val['MI_v_u'] = df_p_val.apply(lambda r: conditional_cdf(fitted_copula, fitted_theta, r.v, r.u),axis=1)\n",
    "    df_p_val.reset_index(inplace=True)\n",
    "    \n",
    "\n",
    "    df = df.merge(df_p_val[['Date','MI_u_v', 'MI_v_u']], on='Date', how='left')\n",
    "    return df.set_index('Date')\n"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"oScy8vuCucjZnEKdqOCA7c",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "FEATURE_LIST_BASE = [\n",
    " 'Div Yield',\n",
    " 'Price to Book',\n",
    " 'Price to Earnings',\n",
    " 'Total_Ret_Pct_1',\n",
    " 'Total_Ret_Pct_5',\n",
    " 'Total_Ret_Pct_21',\n",
    " 'Total_Ret_Pct_63',\n",
    " 'Total_Ret_Pct_252',\n",
    " 'Fast_MACD',\n",
    " 'Med_MACD',\n",
    " 'Slow_MACD',\n",
    " 'Pair_RSI_10',\n",
    " 'Pair_RSI_21',\n",
    " 'Pair_RSI_63',\n",
    " 'Bollinger_10',\n",
    " 'Bollinger_21',\n",
    " 'Bollinger_63',\n",
    " 'zero_cross',\n",
    " 'days_s_cross',\n",
    " 'Real Credit Growth',\n",
    " 'Liquidity Growth',\n",
    " 'Change in Valuation',\n",
    " 'IG Credit Spread',\n",
    " 'HY Credit Spread',\n",
    " 'TED Spread',\n",
    " 'VIX_Roll_diff',\n",
    " 'MI_u_v',\n",
    " 'MI_v_u']\n",
    "\n",
    "FEATURE_LIST = {\n",
    "    'd': FEATURE_LIST_BASE + ['Y_1Fwd_Lagged_1','Y_1Fwd_Lagged_2','Y_1Fwd_Lagged_3','Y_1Fwd_Lagged_4','Y_1Fwd_Lagged_5'],\n",
    "    'w': FEATURE_LIST_BASE + ['Y_5Fwd_Lagged_6','Y_5Fwd_Lagged_7','Y_5Fwd_Lagged_8','Y_5Fwd_Lagged_9','Y_5Fwd_Lagged_10'],\n",
    "    'M': FEATURE_LIST_BASE + ['Y_21Fwd_Lagged_21', 'Y_5Fwd_Lagged_42', 'Y_5Fwd_Lagged_63', 'Y_5Fwd_Lagged_252']\n",
    "}"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lrwC1IxGHRTjYkfDWbOKEx",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# import __file__\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from typing import List, Tuple, Optional, Mapping\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit,  GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "\n",
    "\n",
    "class Predictor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, data: pd.DataFrame, params: Optional[Mapping] = None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, params: Optional[Mapping] = None) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def periodic_train_predict(self, data: pd.DataFrame, params: Optional[Mapping] = None) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "class XGBoostPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    Build XGBoost model\n",
    "    \"\"\"\n",
    "    def __init__(self,freq,):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "        self.freq = freq\n",
    "        self.features = FEATURE_LIST[freq]\n",
    "        self.y_var = f'Y_Fwd_Total_Ret_Pct_{self.freq}'\n",
    "\n",
    "    def train(self, data: pd.DataFrame, freq='d'):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        train_df = data.reset_index(drop=True)\n",
    "        train_X = data[self.features]\n",
    "        train_y = data[self.y_var]\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "        # param_grid = {\n",
    "        #     'learning_rate': [0.1, 0.01,0.001],\n",
    "        #     'max_depth': [1,2,3],\n",
    "        #     'subsample': [0.7, 0.8, 0.9],\n",
    "        #     'n_estimators': [50,100,150]\n",
    "        # }\n",
    "        param_grid = {\n",
    "            # 'n_estimators': [100,200],\n",
    "            'learning_rate': [0.1, 1],\n",
    "            'max_depth': [1,2],\n",
    "            'subsample': [0.8, 1],\n",
    "            'colsample_bytree': [0.8, 1],\n",
    "            'reg_lambda': [0, 1e-2, 1],\n",
    "            'reg_alpha': [0, 1e-2, 1]\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', random_state=10)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=tscv)\n",
    "\n",
    "        grid_search.fit(train_X, train_y)\n",
    "        best_params = grid_search.best_params_\n",
    "        model_best = xgb.XGBRegressor(**best_params)\n",
    "        model_best.fit(train_X, train_y)\n",
    "        self.model = model_best\n",
    "\n",
    "\n",
    "    def predict(self, data: pd.DataFrame, save_path: Optional[str] = None) \\\n",
    "        -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Day by day forecast\n",
    "        \"\"\"\n",
    "        test_X = data[self.features]\n",
    "        test_y = data[self.y_var]\n",
    "\n",
    "        pred_y = self.model.predict(test_X)\n",
    "        \n",
    "        output_df = pd.DataFrame([pred_y, test_y]).T\n",
    "        output_df.columns = [\"pred_spread\", \"actual_spread\"]\n",
    "        output_df.index = data.index\n",
    "        if save_path:\n",
    "            output_df.to_pickle(save_path)\n",
    "        return output_df\n",
    "\n",
    "    def periodic_train_predict(self, data: pd.DataFrame, \\\n",
    "        save_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        pass\n"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"uzRlmezDYpmYPzpiSZ5tOy",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "if __name__ == \"__main__\":\n",
    "    freqs = [\"d\", \"w\", \"M\"]\n",
    "    # modes = [\"predict\", \"periodic_train_predict\"]\n",
    "    modes = [\"predict\"]\n",
    "    \n",
    "    # Set Path\n",
    "    data_path = \"\"\n",
    "    save_path = \"\"\n",
    "    for mode in modes:\n",
    "        if not os.path.exists(f\"{save_path}{mode}\/\"):\n",
    "            os.makedirs(f\"{save_path}{mode}\/\")\n",
    "        if not os.path.exists(f\"{save_path}{mode}_models\/\"):\n",
    "            os.makedirs(f\"{save_path}{mode}_models\/\")\n",
    "\n",
    "    # Load Pairs\n",
    "    raw_price_df = pd.read_csv(data_path + \"price_df.csv\", parse_dates=[\"Date\"])\n",
    "    raw_price_df.sort_values([\"ETF_Ticker\", \"Date\"], inplace=True)\n",
    "    feature_df_raw = pd.read_csv(data_path + \"TrainingSet.csv\", parse_dates=[\"Date\"])\n",
    "    feature_df_raw.sort_values([\"Ticker_Pair\", \"Date\"], inplace=True)\n",
    "    pair_arr = np.unique(feature_df_raw[\"Ticker_Pair\"].values)\n",
    "    \n",
    "    # Make predictions on each pair under \n",
    "    freq_map = {'d':1, 'w':5, 'M':21}\n",
    "    for freq in tqdm(freqs):\n",
    "\n",
    "        price_s = raw_price_df.set_index([\"ETF_Ticker\", \"Date\"]).squeeze()\n",
    "        feature_df = feature_df_raw.set_index([\"Ticker_Pair\", \"Date\"]).squeeze()\n",
    "\n",
    "        for mode in tqdm(modes):\n",
    "            output_list = []\n",
    "            for pair in pair_arr:\n",
    "                print(pair)\n",
    "                pair_list = pair.split(\"_\")\n",
    "                pair_s = price_s.loc[pair_list, :].copy()\n",
    "                pair_df = pair_s.unstack(\"ETF_Ticker\")\n",
    "                pair_df.dropna(inplace=True)\n",
    "\n",
    "                feature_df_pair = feature_df.loc[pair, :].copy()\n",
    "\n",
    "                # Build Predictor\n",
    "                predictor = XGBoostPredictor(freq_map[freq])\n",
    "                if mode == \"predict\":\n",
    "                    feature_df_pair = add_copula_x_variable(feature_df_pair.reset_index(), pair_df[pair_list[0]], pair_df[pair_list[1]],\n",
    "                        train_end=pd.Timestamp('2016-12-31'),ret_days=freq_map[freq])\n",
    "                    feature_df_pair['VIX_Roll_diff'] = feature_df_pair['VIX Roll'].diff()\n",
    "                    train_df = feature_df_pair.loc[feature_df_pair.index < pd.Timestamp(\"2017-01-01\")].copy().dropna()\n",
    "                    test_df = feature_df_pair.loc[feature_df_pair.index >= pd.Timestamp(\"2017-01-01\")].copy()\n",
    "                    predictor.train(train_df)\n",
    "                    pickle.dump(predictor.model, open(f\"{save_path}{mode}_models\/xgb_{freq}_{pair}.pkl\", \"wb\"))\n",
    "\n",
    "                    df_pred_train = predictor.predict(train_df)\n",
    "                    df_pred_test = predictor.predict(test_df)\n",
    "                    output_i_df = pd.concat([df_pred_train, df_pred_test])\n",
    "               \n",
    "                output_i_df.index.name = \"Date\"\n",
    "                output_i_df.reset_index(inplace=True)\n",
    "                output_i_df[\"pair\"] = pair\n",
    "                output_i_df = output_i_df.reindex(\n",
    "                    [\"pair\"] + list(output_i_df.columns[:-1]), axis=1\n",
    "                )\n",
    "                output_list.append(output_i_df)\n",
    "                \n",
    "            output_df = pd.concat(output_list)\n",
    "            output_df.to_pickle(f\"{save_path}{mode}\/ReturnSpreadPredictions_{freq}.pkl\")"
   ],
   "execution_count":5,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "*XFN_IJH\n",
      "Already there\n",
      "*XIC_EZA\n",
      "Already there\n",
      "*XIU_REM\n",
      "Already there\n",
      "BKF_IGM\n",
      "Already there\n",
      "BKF_IVV\n",
      "Already there\n",
      "EWD_THD\n",
      "Already there\n",
      "EWG_THD\n",
      "Already there\n",
      "EWH_IJH\n",
      "Already there\n",
      "EWM_EZA\n",
      "Already there\n",
      "EWQ_IYG\n",
      "Already there\n",
      "EWT_IJH\n",
      "Already there\n",
      "EWT_IJJ\n",
      "Already there\n",
      "EWT_IVE\n",
      "Already there\n",
      "EWT_IVV\n",
      "Already there\n",
      "EWT_IWB\n",
      "Already there\n",
      "EWT_IWV\n",
      "Already there\n",
      "EZU_THD\n",
      "Already there\n",
      "IDU_IVV\n",
      "Already there\n",
      "IEV_THD\n",
      "Already there\n",
      "IFGL_IHE\n",
      "IFGL_IWM\n",
      "IGM_IHI\n",
      "IHE_ACWI\n",
      "IHE_ACWX\n",
      "IHE_SCZ\n",
      "IHE_TOK\n",
      "IHE_WOOD\n",
      "IHF_ILCB\n",
      "IHF_ILCG\n",
      "IHF_IUSG\n",
      "IHF_IVW\n",
      "IHF_IXN\n",
      "IJH_ACWI\n",
      "IJH_TOK\n",
      "IJH_WOOD\n",
      "IJJ_ACWI\n",
      "IMCV_IWM\n",
      "IVV_ACWI\n",
      "IWB_ACWI\n",
      "IWM_ACWI\n",
      "IWM_SCZ\n",
      "IWM_SUSA\n",
      "IWM_WOOD\n",
      "IWV_ACWI\n",
      "IXP_IYH\n",
      "IYH_ACWI\n",
      "IYH_TOK\n",
      "IYK_ACWI\n",
      "REM_SUSA\n",
      "RXI_WOOD\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "\r  0%|          | 0\/1 [00:00<?, ?it\/s]\n",
      "\r  0%|          | 0\/1 [00:00<?, ?it\/s]\u001b[A\n",
      "\r100%|██████████| 1\/1 [1:38:57<00:00, 5937.24s\/it]\u001b[A\r100%|██████████| 1\/1 [1:38:57<00:00, 5937.25s\/it]\n",
      "\r100%|██████████| 1\/1 [1:38:57<00:00, 5937.38s\/it]\r100%|██████████| 1\/1 [1:38:57<00:00, 5937.39s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"c7xcxLC2eeF6mA31adbKzK",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}